from pathlib import Path
import shutil
import pandas as pd
import argparse
import glob

### Load samples.tsv file and obtain list of samples ###
SAMPLE_TABLE = pd.read_csv(config["input"]["sample table"], index_col="sample", sep="\t")
SAMPLE_TABLE.index = SAMPLE_TABLE.index.map(str)  # convert index (samples) to string 
SAMPLES = SAMPLE_TABLE.index.tolist()  # obtain list of samples 

##### Define intermediate/results files/directories #####
scratch_dir = Path(config["scratch directory"])
results_dir = Path(config["results directory"])

Path(scratch_dir).mkdir(exist_ok=True, parents=True)
Path(results_dir).mkdir(exist_ok=True, parents=True)

scratch_dict = {
    # read trimming 
    "trimmed_reads": scratch_dir / "trimmed_reads",

    # kaiju classification 
    "classified_kaiju_read_output": scratch_dir / "classified_kaiju_read_output",

    # Classified Pro Syn reads
    "prosyn_reads": {
        # dir of read name classified as Pro or Syn 
        "read_name": scratch_dir / "prosyn_reads" / "read_name", 
        # dir of read name classified as Pro or Syn along with their full taxonomic classification 
        "read_name_classification": scratch_dir / "prosyn_reads" / "read_name_classification", 
        # dir of extracted fastq sequences of Pro/Syn reads
        "extracted_reads": scratch_dir / "prosyn_reads" / "extracted_reads", 
    }, 

    # diamond blast binned reads 
    "diamond_blast": scratch_dir / "diamond_blast",

    # read count normalization
    "count_normalization": {
        # dir of sample directories containing normalized count files for each clade
        "normalized_counts": scratch_dir / "count_normalization" / "normalized_counts",
        # directory to store aggreated normalized counts (1 file per sample)
        "aggregated_normalized": scratch_dir / "count_normalization" / "aggregated_normalized",
    }, 
}

results_dict = {
    # read count and percentages of specified genus in config
    "summary_read_count": results_dir / "summary_read_count.tsv", 
    # normalized counts (genome equivalents) of Pro and Syn clades 
    "final_normalized_count": results_dir / "normalized_counts.tsv", 
}

##### Define the file files to generate #####
rule all:
    input:
        results_dict['final_normalized_count'],  
        results_dict['summary_read_count'], 

##### Define the Rules that are used in this pipeline #####
include: "rules/run_kaiju.smk"
include: "rules/extract_reads.smk"
include: "rules/blast_reads.smk"
include: "rules/normalize_reads.smk"
include: "rules/aggregate_results.smk"
