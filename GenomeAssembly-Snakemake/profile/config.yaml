snakefile: workflow/Snakefile
use-conda: True
conda-frontend: mamba
rerun-incomplete: True
jobs: 3  # number of jobs (samples) to process at once 
latency-wait: 120
keep-going: True
configfile: inputs/config.yaml
keep-incomplete: False
# unlock: True
# dry-run: True

cluster: 
  mkdir -p logs/{rule} &&
  sbatch
    --partition={resources.partition}
    --ntasks={resources.tasks}
    --cpus-per-task={resources.cpus_per_task}
    --mem={resources.mem}
    --time={resources.time}
    --job-name={rule}-%j
    --output="logs/{rule}/{wildcards}.out"
    --error="logs/{rule}/{wildcards}.err"

# adjust as needed (increase time and mem if files are large)
# most rules are multi-threaded 
default-resources: 
  - time="1-0"  # default time 
  - partition="sched_mit_chisholm"
  - mem=100000  # default memory 
  - cpus_per_task=10
  - tasks=1

# for aggregate rules that aren't mutli-threaded
set-resources:
  - aggregate_results:cpus_per_task=1
  - parse_fastANI_Prochlorococcus:cpus_per_task=1
  - finalize_annot:cpus_per_task=1